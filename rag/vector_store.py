import logging
import time
import os
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
# Importation du StorageManager corrig√©
from services.utils import StorageManager 
# Utilisation de l'ancienne classe Logger pour ne pas recr√©er la config de logging
# logger = logging.getLogger("scraper.orchestrator") 
# logging.basicConfig(level=logging.INFO)

# --- Configuration du Logging (Simplifi√©e) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("ScraperOrchestrator")


# --- Stubs pour les composants RAG manquants (pour que le script tourne) ---
class VectorStoreHandler:
    def search(self, query_vector: list, k: int, source_filter: str, vector_filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        return [{"content": f"R√©sultat vectoriel pour {source_filter}", "score": 0.9}]

    def index_data(self, category: str, data: Dict[str, Any]):
        logging.getLogger("VectorStoreHandler").debug(f"Indexation simul√©e pour {category}.")
        pass

class Reranker:
    def rerank(self, query: str, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        logging.getLogger("Reranker").debug("Reranking simul√©.")
        return results

class EmbeddingService:
    def embed_query(self, query: str) -> list:
        logging.getLogger("EmbeddingService").debug("Embedding de requ√™te simul√©.")
        return [0.1] * 128


# --- Interface d'Agent (Pour le d√©couplage) ---
class ScraperAgent:
    def __init__(self, category: str):
        self.category = category 

    def run(self, zone_id: str) -> List[Dict[str, Any]]:
        """Simule l'ex√©cution de l'agent pour une zone sp√©cifique."""
        logger.info(f"‚öôÔ∏è Ex√©cution de l'agent [{self.category}] pour la zone {zone_id}...")
        
        # --- Simule la collecte de donn√©es vari√©es (robustesse) ---
        if self.category == 'METEO':
            # La m√©t√©o change √† chaque heure, donc le hash changera
            now = datetime.now()
            return [{
                "time_prevision": (now + timedelta(hours=i)).isoformat(),
                "temp_c": 20.0 + (i * 0.1) + (now.minute / 100), # Change l√©g√®rement pour tester le hash
                "description": "Bulletin M√©t√©orologique pour la journ√©e",
                "source_url": f"http://meteo.com/bulletin/{zone_id}"
            } for i in range(2)]
        elif self.category == 'SUBVENTION':
            # La subvention reste stable (m√™me hash) pour la d√©duplication
            return [{
                "grant_id": "S999",
                "title": "Aide Agricole Urgente",
                "deadline": (datetime.now() + timedelta(days=60)).isoformat(),
                "amount_eur": 50000.0,
                "eligible_zones": [zone_id],
                "source_url": "http://subventions.gouv/agri"
            }]
        elif self.category == 'ALERTE_INONDATION':
            # Alerte stable pour la d√©mo
            return [{
                "level": "Rouge",
                "risk_area": zone_id,
                "timestamp": datetime.now().isoformat(),
                "details": "Niveau de crue critique sur le fleuve.",
                "source_url": "http://alertes.gouv/inondation"
            }]
        else:
            return []

class ScraperOrchestrator:
    """G√®re la cha√Æne d'ex√©cution des agents de scraping et collecte les r√©sultats bruts."""

    def __init__(self, agents: Dict[str, ScraperAgent], zones: List[str]):
        self.agents = agents 
        self.zones = zones 
        logger.info(f"üåê Orchestrateur initialis√©. {len(self.agents)} agents pour {len(self.zones)} zones.")
    
    def run_agent_and_collect(self, category: str, agent: ScraperAgent) -> List[Dict[str, Any]]:
        """Ex√©cute un agent pour toutes les zones et retourne la liste brute des r√©sultats."""
        collected_data = []
        logger.info(f"\n--- D√©marrage de l'agent : {category} ---")
        
        for zone_id in self.zones:
            try:
                raw_results = agent.run(zone_id)
                for result in raw_results:
                    # Enrichir la donn√©e brute avec les m√©tadonn√©es de l'ex√©cution
                    collected_data.append({
                        "category": category,
                        "zone_id": zone_id,
                        "data": result,
                        "acquisition_time": time.time(), 
                    })
                
                logger.info(f"‚úÖ Collecte r√©ussie pour {zone_id} : {len(raw_results)} enregistrements.")

            except Exception as e:
                logger.error(f"‚ùå Erreur critique de l'agent {category} pour {zone_id}: {e}")

        return collected_data

    def run_pipeline(self) -> List[Dict[str, Any]]:
        """Lance l'ex√©cution de tous les agents et retourne tous les r√©sultats collect√©s."""
        all_collected_data = []
        for category, agent in self.agents.items():
            results = self.run_agent_and_collect(category, agent)
            all_collected_data.extend(results)
        return all_collected_data


# --- D√âMONSTRATION DE LA PIPELINE D√âCOUPL√âE ---
if __name__ == '__main__':
    # Initialisation des composants
    DB_PATH = "data/orchestrator_final.db"
    
    # 1. PR√âPARATION DES OUTILS (StorageManager pour la persistance)
    try:
        # Assurez-vous que le StorageManager est bien instanci√© apr√®s la correction
        storage = StorageManager(db_path=DB_PATH)
        store = VectorStoreHandler()
        embedder = EmbeddingService()
        reranker = Reranker()
    
    except Exception as e:
        logger.error(f"Erreur fatale d'initialisation des services: {e}. V√©rifiez storage_manager.py.")
        exit()

    # 2. D√âFINITION DE LA T√ÇCHE
    agents_map = {
        "METEO": ScraperAgent("METEO"),
        "SUBVENTION": ScraperAgent("SUBVENTION"),
        "ALERTE_INONDATION": ScraperAgent("ALERTE_INONDATION"),
    }
    zones_list = ["Paris", "Lyon", "Marseille"]

    # 3. L'ORCHESTRATEUR (Ex√©cution pure)
    orchestrator = ScraperOrchestrator(agents_map, zones_list)
    
    print("\n[√âtape 1] üöÄ Lancement de l'Orchestrateur pour collecter les donn√©es...")
    final_collected_data = orchestrator.run_pipeline() 
    
    print(f"\n[√âtape 1 Termin√©] Total des enregistrements collect√©s par l'Orchestrateur : {len(final_collected_data)}")

    # 4. LE FLUX DE TRAITEMENT AVAL (Persistance, Caching, Vectorisation)
    print("\n[√âtape 2] üíæ D√©marrage du Traitement Aval (Persistence et Caching/D√©duplication)...")
    
    processed_count = 0
    for item in final_collected_data:
        # Stockage de la donn√©e brute dans la table dynamique
        is_new = storage.save_raw_data(
            zone_id=item["zone_id"],
            category=item["category"],
            data=item["data"],
            effective_date=item["acquisition_time"],
            source_url=item["data"].get("source_url")
        )
        
        # Le m√™me 'item' serait envoy√© √† un Vector Store SEULEMENT s'il est nouveau ou modifi√©
        if is_new:
            store.index_data(item["category"], item["data"]) 
            processed_count += 1

    print(f"\n[√âtape 2 Termin√©] Total des NOUVEAUX enregistrements persist√©s (apr√®s d√©duplication) : {processed_count}")

    # 5. Testons la robustesse/d√©duplication en relan√ßant l'Orchestrateur
    print("\n[√âtape 3] üîÑ Relance de la pipeline (pour tester la d√©duplication)...")
    second_run_data = orchestrator.run_pipeline()
    processed_count_second = 0
    for item in second_run_data:
        is_new = storage.save_raw_data(
            zone_id=item["zone_id"],
            category=item["category"],
            data=item["data"],
            effective_date=item["acquisition_time"],
            source_url=item["data"].get("source_url")
        )
        if is_new: 
            processed_count_second += 1

    # Attente pour s'assurer que les logs de la base de donn√©es sont clairs
    time.sleep(0.5) 

    print("\n[√âtape 3 Termin√©] Analyse des r√©sultats :")
    print(f"- Total collect√© (2e ex√©cution) : {len(second_run_data)}")
    print(f"- Total des NOUVEAUX enregistrements persist√©s (doit √™tre bas) : {processed_count_second}")

    # Testons le 'retrieve facile' du cache pour les donn√©es critiques
    print("\n[Test Retrieve Facile] üîç R√©cup√©ration des donn√©es d'alerte √† Lyon via Cache...")
    alertes_lyon = storage.get_raw_data(zone_id="Lyon", category="ALERTE_INONDATION", limit=1)
    if alertes_lyon:
        print(f"-> R√©sultat du Cache (Alerte Lyon) : Niveau '{alertes_lyon[0].get('level')}'")
    else:
        print("-> Aucune alerte trouv√©e dans le cache.")

    storage.close()