# orchestrator/orchestrator.py
import os
import sys
import logging
from typing import TypedDict, List, Dict, Any, Optional

# --- Configuration du chemin pour les imports locaux ---
# Permet d'importer les modules depuis le dossier parent
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PARENT_DIR = os.path.dirname(CURRENT_DIR)
if PARENT_DIR not in sys.path:
    sys.path.append(PARENT_DIR)

# --- Imports LangChain/LangGraph ---
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# --- Imports Modules Locaux (Architecture Modulaire) ---
from orchestrator.intention import IntentClassifier
from orchestrator.central_data_manager import CentralDataManager
# Assurez-vous que ces fichiers existent ou commentez les imports si test partiel
try:
    from agents.Meteo import MeteoAgent  # Suppose que MeteoAgent a une m√©thode get_graph()
    from agents.Crop import BurkinaCropAgent
    from agents.Soil import SoilManagementService
    from agents.Health import HealthManagementService
    from agents.subsidy import SubsidyManagementService
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Certains agents n'ont pas pu √™tre import√©s : {e}")

# --- Configuration Logging ---
logger = logging.getLogger("Orchestrator")
logger.setLevel(logging.INFO)

# ======================================================================
# 1. D√âFINITION DE L'√âTAT GLOBAL (Le "Bus" de donn√©es)
# ======================================================================
class OrchestratorState(TypedDict):
    """
    √âtat partag√© qui circule entre tous les n≈ìuds du graphe.
    Contient l'intention, les donn√©es contextuelles et la r√©ponse finale.
    """
    user_id: str
    zone_id: str
    user_query: str
    intent: str
    
    # Context Data (Inject√© par CentralDataManager)
    meteo_data: Optional[Dict]
    culture_config: Optional[Dict]
    soil_config: Optional[Dict]
    user_profile: Optional[Dict]

    # Sorties et Tra√ßabilit√©
    final_response: str
    execution_trace: List[str] # Pour le debugging et l'explicabilit√©


# ======================================================================
# 2. ORCHESTRATEUR PRINCIPAL ("The Boss")
# ======================================================================
class AgriculturalOrchestrator:
    
    OLLAMA_MODEL = "mistral"
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        self.ollama_url = ollama_url
        
        # 1. Initialisation du Client LLM Central
        # On partage ce client avec les agents pour √©conomiser les ressources
        self.ollama_client = self._init_llm_client()
        
        # 2. Initialisation des Services Core
        self.classifier = IntentClassifier(model_name=self.OLLAMA_MODEL)
        self.data_manager = CentralDataManager()

        # 3. Chargement des Agents Sp√©cialis√©s
        # Chaque agent est une "bo√Æte noire" autonome
        self.agents = {}
        self._load_agents()

    def _init_llm_client(self):
        """Tente de connecter Ollama avec une gestion d'erreur robuste."""
        try:
            client = ChatOllama(
                model=self.OLLAMA_MODEL, 
                base_url=self.ollama_url,
                temperature=0.1 # Basse temp√©rature pour la pr√©cision technique
            )
            # Ping test
            client.invoke("Hi")
            logger.info(f"‚úÖ Master Orchestrator connect√© √† Ollama ({self.OLLAMA_MODEL})")
            return client
        except Exception as e:
            logger.error(f"‚ùå CRITIQUE : Ollama indisponible. Le mode d√©grad√© sera activ√©. Erreur: {e}")
            return None

    def _load_agents(self):
        """Charge dynamiquement les agents disponibles."""
        # On passe le client LLM aux agents pour √©viter qu'ils ne r√©instancient chacun une connexion
        try:
            self.agents["METEO"] = MeteoAgent()
            self.agents["CROP"] = BurkinaCropAgent()
            self.agents["SOIL"] = SoilManagementService()
            self.agents["HEALTH"] = HealthManagementService()
            self.agents["SUBSIDY"] = SubsidyManagementService() # Subsidy g√®re son client diff√©remment dans ton code pr√©c√©dent
            logger.info(f"‚úÖ {len(self.agents)} Agents charg√©s avec succ√®s.")
        except NameError:
            logger.warning("‚ö†Ô∏è Certains agents ne sont pas d√©finis (NameError). V√©rifiez les imports.")

    # ============================================================
    # NODES (Les √©tapes du processus)
    # ============================================================

    def classify_node(self, state: OrchestratorState) -> OrchestratorState:
        """√âtape 1 : Comprendre ce que veut l'utilisateur."""
        query = state.get("user_query", "")
        
        # Utilisation du Classifier Robuste (LLM + Fallback Regex)
        intent = self.classifier.predict(query)
        
        trace = state.get("execution_trace", []) + [f"Intent Detected: {intent}"]
        logger.info(f"üß† Classification: {intent}")
        
        return {**state, "intent": intent, "execution_trace": trace}

    def retrieve_node(self, state: OrchestratorState) -> OrchestratorState:
        """√âtape 2 : R√©cup√©rer les munitions (donn√©es) pour l'agent."""
        if state["intent"] == "UNKNOWN":
            return state

        # Le DataManager sait quelles donn√©es aller chercher selon l'intention
        context_data = self.data_manager.retrieve_context(state)
        
        # On fusionne les nouvelles donn√©es dans l'√©tat
        new_state = {**state, **context_data}
        
        keys_found = [k for k, v in context_data.items() if v is not None]
        trace = state["execution_trace"] + [f"Context Loaded: {keys_found}"]
        
        return {**new_state, "execution_trace": trace}

    def dispatch_node(self, state: OrchestratorState) -> OrchestratorState:
        """√âtape 3 : D√©l√©guer √† l'Expert (Agent)."""
        intent = state["intent"]
        trace = state["execution_trace"]
        
        agent_service = self.agents.get(intent)
        
        if not agent_service:
            logger.error(f"Agent {intent} not found in registry.")
            return {
                **state, 
                "final_response": "D√©sol√©, le service demand√© est momentan√©ment indisponible.",
                "execution_trace": trace + ["Error: Agent missing"]
            }

        logger.info(f"üöÄ Dispatching to Agent: {intent}")
        
        try:
            # Invocation du graphe de l'agent
            # L'agent re√ßoit tout l'√©tat, fait son travail, et retourne son √©tat local
            agent_result = agent_service.get_graph().invoke(state)
            
            # Extraction de la r√©ponse finale de l'agent
            response = agent_result.get("final_response", "L'agent n'a pas retourn√© de r√©ponse.")
            status = agent_result.get("status", "UNKNOWN_STATUS")
            
            return {
                **state,
                "final_response": response,
                "execution_trace": trace + [f"Agent Execution: SUCCESS ({status})"]
            }

        except Exception as e:
            logger.error(f"üí• Error inside Agent {intent}: {e}", exc_info=True)
            return {
                **state,
                "final_response": f"Une erreur technique est survenue lors de l'analyse ({intent}). Veuillez r√©essayer.",
                "execution_trace": trace + [f"Agent Crash: {str(e)}"]
            }

    def fallback_node(self, state: OrchestratorState) -> OrchestratorState:
        """√âtape Secours : Si l'intention est inconnue."""
        query = state["user_query"]
        trace = state["execution_trace"] + ["Fallback: General LLM"]
        
        if not self.ollama_client:
            return {**state, "final_response": "Je suis hors ligne. Veuillez v√©rifier ma connexion.", "execution_trace": trace}

        # Prompt optimis√© pour √™tre utile m√™me en cas d'incompr√©hension, avec gestion des DIAGRAMMES
        system_prompt = (
            "Tu es un assistant agricole intelligent. L'utilisateur a pos√© une question qui ne correspond "
            "pas √† nos cat√©gories standards (M√©t√©o, Sol, Culture, Sant√©, Subventions). "
            "1. R√©ponds poliment et essaie d'aider si le sujet reste agricole (ex: machinerie, √©levage). "
            "2. Si la question est hors-sujet, redirige-le vers l'agriculture. "
            "3. Si l'explication b√©n√©ficie d'un sch√©ma visuel (ex: anatomie d'une vache, pi√®ce de tracteur), "
            "utilise le tag. Sois √©conome avec les images, utilise-les seulement si instructif."
        )

        try:
            response = self.ollama_client.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=query)
            ])
            return {**state, "final_response": response.content, "execution_trace": trace}
        except Exception as e:
            return {**state, "final_response": "Je n'ai pas compris votre demande.", "execution_trace": trace + ["Fallback Error"]}

    # ============================================================
    # 3. CONSTRUCTION DU GRAPHE (ROUTAGE)
    # ============================================================
    
    def get_graph(self):
        workflow = StateGraph(OrchestratorState)

        # Ajout des n≈ìuds
        workflow.add_node("classify", self.classify_node)
        workflow.add_node("retrieve", self.retrieve_node)
        workflow.add_node("dispatch", self.dispatch_node)
        workflow.add_node("fallback", self.fallback_node)

        # Point d'entr√©e
        workflow.set_entry_point("classify")

        # Logique de branchement conditionnel
        def route_intent(state):
            intent = state.get("intent", "UNKNOWN")
            if intent in ["METEO", "CROP", "SOIL", "HEALTH", "SUBSIDY"]:
                return "retrieve"
            return "fallback"

        workflow.add_conditional_edges(
            "classify",
            route_intent,
            {
                "retrieve": "retrieve",
                "fallback": "fallback"
            }
        )

        # Flux lin√©aire pour les cas connus
        workflow.add_edge("retrieve", "dispatch")
        workflow.add_edge("dispatch", END)
        workflow.add_edge("fallback", END)

        return workflow.compile()

# ======================================================================
# 4. EX√âCUTION DE TEST (SIMULATION)
# ======================================================================
if __name__ == "__main__":
    # Setup pour le visuel console
    logging.basicConfig(level=logging.INFO, format='%(name)s - %(message)s')
    
    print("\nüöú INITIALISATION DE L'ORCHESTRATEUR AGRICOLE...")
    orchestrator = AgriculturalOrchestrator()
    app = orchestrator.get_graph()

    def run_simulation(query: str, zone: str = "Koudougou"):
        print(f"\n{'='*60}")
        print(f"üë§ USER ({zone}): {query}")
        print(f"{'='*60}")
        
        initial_state = {
            "user_id": "sim_user_01",
            "zone_id": zone,
            "user_query": query,
            # Le reste est initialis√© √† vide ou None
            "intent": "", "final_response": "", "execution_trace": [],
            "meteo_data": None, "culture_config": None, "soil_config": None, "user_profile": None
        }
        
        result = app.invoke(initial_state)
        
        print(f"\nü§ñ BOT RESPONSE:\n{result['final_response']}")
        print(f"\nüîç TRACE: {' -> '.join(result['execution_trace'])}")

    # TEST 1 : Cas Complexe (Sant√© + M√©t√©o implicite via DataManager)
    run_simulation("Les feuilles de mon ma√Øs jaunissent et il y a des taches. Que faire ?")

    # TEST 2 : Cas Subvention (Avec visuel attendu dans l'agent)
    run_simulation("C'est quoi la proc√©dure pour avoir l'engrais subventionn√© ?")
    
    # TEST 3 : Cas Fallback (Machinerie - doit d√©clencher le LLM g√©n√©raliste + Image potentielle)
    run_simulation("Comment fonctionne un moteur diesel de tracteur ?")