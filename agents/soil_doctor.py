import logging
from typing import TypedDict, Dict, Any, Optional
from langgraph.graph import StateGraph, END

# --- Importations LangChain ---
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_community.chat_models import ChatOllama

# --- Outil M√©tier ---
from tools.soils.base_soil import SoilDoctorTool

# ======================================================================
# 1. D√âFINITION DE L'√âTAT
# ======================================================================
class AgentState(TypedDict):
    user_query: str
    soil_config: Dict[str, Any]
    technical_advice_raw: Optional[str]
    final_response: str
    status: str

# ======================================================================
# 2. SERVICE DE GESTION DES SOLS (DOCTEUR SOL)
# ======================================================================
class SoilDoctor:
    OLLAMA_MODEL = "mistral"

    def __init__(self, ollama_host: str = "http://localhost:11434", llm_client=None):
        self.pedologist = SoilDoctorTool()
        self.logger = logging.getLogger("agent.soil")
        
        try:
            self.llm_client = llm_client if llm_client else ChatOllama(
                model=self.OLLAMA_MODEL, 
                base_url=ollama_host, 
                temperature=0.1
            )
        except Exception as e:
            self.llm_client = None
            self.logger.warning(f"Ollama indisponible. Mode fallback activ√©. Erreur: {e}")

    # --- N≈íUD 1 : ANALYSE TECHNIQUE ---
    def analyze_node(self, state: AgentState) -> Dict[str, Any]:
        query = state.get("user_query", "").lower()
        config = state.get("soil_config", {})
        
        # --- FEATURE : LE FUMIER PI√âG√â (Compost Tracker) ---
        if any(word in query for word in ["fumier", "compost", "engrais organique"]):
            response_text = (
                "üí© **TRACKER DE COMPOST ET FUMIER**\n\n"
                "Le fumier est l'or brun du paysan, mais mal utilis√©, il br√ªle les racines.\n"
                "**Signes de maturit√© :**\n"
                "* **Odeur :** Terre de for√™t humide (pas d'ammoniac).\n"
                "* **Texture :** D√©bris v√©g√©taux m√©connaissables.\n"
                "* **Temp√©rature :** Le tas doit √™tre froid au toucher.\n\n"
                "\n\n"
                "‚ö†Ô∏è **CONSEIL :** Si votre compost d√©gage encore de la chaleur, il est 'en feu'. Attendez 3 √† 4 semaines."
            )
            return {"technical_advice_raw": response_text, "status": "COMPOST_ADVICE"}

        # --- ANALYSE DE SOL STANDARD ---
        texture = config.get("texture", "sableux")
        ph = float(config.get("ph", 6.5))
        budget = config.get("budget", "bas")

        diagnosis = self.pedologist.get_full_diagnosis(texture=texture, obs_text=query, ph=ph)
        
        if "error" in diagnosis:
            return {"technical_advice_raw": f"Erreur: {diagnosis['error']}", "status": "ERROR"}

        p_source = self.pedologist.recommend_p_source(budget=budget)
        ces_tech = diagnosis['ces_recommendation']['technique']

        # Construction du rapport avec injection de sch√©mas techniques
        raw_report = (
            f"TYPE DE SOL : {diagnosis['soil_type']}\n"
            f"√âTAT HYDRIQUE : {diagnosis['moisture_status']}\n"
            f"ANALYSES : {diagnosis['ph_analysis']}\n"
            f"TECHNIQUE ANTI-√âROSION : {ces_tech}\n"
            f"CONSEIL NUTRITION : {p_source}\n"
        )

        # Ajout des sch√©mas explicatifs selon la technique
        if "Za√Ø" in ces_tech:
            raw_report += "\n"
        elif "Cordon" in ces_tech:
            raw_report += "\n"
        elif "Demi-lune" in ces_tech:
            raw_report += "\n"
        elif "Billonnage" in ces_tech:
            raw_report += "\n"

        return {"technical_advice_raw": raw_report, "status": "TECHNICAL_DONE"}

    # --- N≈íUD 2 : FORMATAGE LLM ---
    def format_node(self, state: AgentState) -> Dict[str, Any]:
        raw_advice = state.get("technical_advice_raw", "")
        
        # Si c'est d√©j√† un conseil compost ou si pas de LLM, on renvoie brut
        if state["status"] == "COMPOST_ADVICE" or not self.llm_client:
            return {"final_response": raw_advice, "status": "SUCCESS"}

        system_prompt = (
            "Tu es l'Architecte du Sol d'AgriConnect.\n"
            "Ton but est de transformer des donn√©es techniques en une ordonnance claire pour un paysan.\n"
            "Respecte scrupuleusement les balises pr√©sentes dans le texte, elles sont vitales.\n\n"
            "STYLE : Chaleureux, expert, imag√©.\n"
            "STRUCTURE :\n"
            "1. üåç L'√âTAT DE TON CHAMP (Texture, pH).\n"
            "2. üèóÔ∏è LES TRAVAUX DE TERRE (Za√Ø, Cordons, etc. avec leur sch√©ma).\n"
            "3. üíä LA RECETTE DE NUTRITION (Compost + Min√©ral).\n"
            "4. ‚ö†Ô∏è LE POINT DE VIGILANCE."
        )

        try:
            response = self.llm_client.invoke([
                SystemMessage(content=system_prompt),
                HumanMessage(content=f"Rapport technique :\n{raw_advice}")
            ])
            return {"final_response": response.content, "status": "SUCCESS"}
        except Exception:
            return {"final_response": raw_advice, "status": "FALLBACK"}

    def get_graph(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("analyze", self.analyze_node)
        workflow.add_node("format", self.format_node)
        workflow.set_entry_point("analyze")
        workflow.add_edge("analyze", "format")
        workflow.add_edge("format", END)
        return workflow.compile()