# orchestrator.py
import logging
import time
import json
import os
import signal
from datetime import datetime
from typing import Dict, Any, Callable, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError

import sys
# Ajout du root au path pour trouver config.py
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import config

# Importer tes services (adapter si signatures diff√©rentes)
from services.scraper import DocumentScraper, WeatherForecastService, SonagessScraper

# Logger
logger = logging.getLogger("scraper.orchestrator")
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Param√®tres d'orchestration
MAX_WORKERS = getattr(config, "ORCHESTRATOR_MAX_WORKERS", 3)
TASK_TIMEOUT_S = getattr(config, "ORCHESTRATOR_TASK_TIMEOUT_S", 300)  # timeout par t√¢che
RETRY_ATTEMPTS = getattr(config, "ORCHESTRATOR_RETRY_ATTEMPTS", 3)
RETRY_BACKOFF_BASE = getattr(config, "ORCHESTRATOR_RETRY_BACKOFF_BASE", 2)  # seconds
REPORT_FILE = getattr(config, "ORCHESTRATOR_REPORT_FILE", "orchestration_report.json")


def _normalize_result(res: Any) -> Dict[str, Any]:
    """
    Normalise la sortie d'une t√¢che en dict:
    { status: "SUCCESS"|"ERROR", results: list, error: optional str }
    """
    if isinstance(res, dict):
        status = res.get("status", "SUCCESS" if res.get("results") is not None else "ERROR")
        results = res.get("results", [])
        error = res.get("error")
        return {"status": status, "results": results or [], "error": error}
    # si la fonction renvoie une liste ou autre
    if isinstance(res, list):
        return {"status": "SUCCESS", "results": res, "error": None}
    if res is None:
        return {"status": "ERROR", "results": [], "error": "No result returned"}
    return {"status": "SUCCESS", "results": [res], "error": None}


def _safe_call_retryable(fn: Callable, *args, attempts: int = RETRY_ATTEMPTS, backoff_base: int = RETRY_BACKOFF_BASE, **kwargs) -> Dict[str, Any]:
    """
    Appelle fn(*args, **kwargs) avec retries simples et backoff exponentiel.
    Retourne un dict normalis√©.
    """
    last_exc = None
    for attempt in range(1, attempts + 1):
        try:
            start = time.time()
            res = fn(*args, **kwargs)
            duration = time.time() - start
            logger.info("Task %s succeeded in %.2fs (attempt %d/%d)", getattr(fn, "__name__", str(fn)), duration, attempt, attempts)
            return _normalize_result(res)
        except Exception as e:
            last_exc = e
            wait = backoff_base ** (attempt - 1)
            logger.warning("Task %s failed on attempt %d/%d: %s. Backing off %ds", getattr(fn, "__name__", str(fn)), attempt, attempts, e, wait)
            time.sleep(wait)
    logger.error("Task %s failed after %d attempts: %s", getattr(fn, "__name__", str(fn)), attempts, last_exc)
    return {"status": "ERROR", "results": [], "error": str(last_exc)}


class ScraperOrchestrator:
    def __init__(self, headless: bool = True):
        self.headless = headless
        # Instancier les services (adapter les signatures si besoin)
        self.document_scraper = DocumentScraper(headless=headless)
        self.weather_service = WeatherForecastService(headless=headless)
        self.sonagess_scraper = SonagessScraper(
            start_url=getattr(config, "START_URL", config.START_URL),
            download=True,
            max_depth=getattr(config, "MAX_DEPTH", config.MAX_DEPTH),
            max_pages=getattr(config, "MAX_PAGES", config.MAX_PAGES),
            output_dir=getattr(config, "OUTPUT_DIR", config.OUTPUT_DIR),
        )

        self.report: Dict[str, Any] = {}
        self._stop_requested = False
        # installer handler pour arr√™t propre
        signal.signal(signal.SIGINT, self._handle_stop)
        signal.signal(signal.SIGTERM, self._handle_stop)

    def _handle_stop(self, signum, frame):
        logger.warning("Stop signal re√ßu (%s). Arr√™t propre demand√©.", signum)
        self._stop_requested = True

    def _run_task_with_timeout(self, executor: ThreadPoolExecutor, fn: Callable, *args, task_name: str = "task", timeout_s: int = TASK_TIMEOUT_S, **kwargs) -> Dict[str, Any]:
        """
        Soumet la t√¢che au ThreadPoolExecutor et attend avec timeout.
        Utilise _safe_call_retryable pour retries internes.
        """
        future = executor.submit(_safe_call_retryable, fn, *args, **kwargs)
        try:
            result = future.result(timeout=timeout_s)
            # Sauvegarde locale imm√©diate (m√™me si cette m√©thode n'est pas utilis√©e par run_all, c'est une bonne pratique)
            self._save_local_cache(task_name, result)
            return result
        except TimeoutError:
            logger.error("Timeout: la t√¢che %s a d√©pass√© %ds", task_name, timeout_s)
            return {"status": "ERROR", "results": [], "error": f"Timeout after {timeout_s}s"}
        except Exception as e:
            logger.exception("Exception inattendue lors de l'ex√©cution de %s: %s", task_name, e)
            return {"status": "ERROR", "results": [], "error": str(e)}

    def _save_local_cache(self, task_name: str, data: Dict[str, Any]):
        """Persiste les donn√©es brutes pour les Agents (Offline-First)."""
        try:
            # Cr√©ation du dossier 'data/cache' pour ne pas polluer la racine 'data/' si n√©cessaire
            # Mais par simplicit√© on met tout dans 'data/' comme demand√©
            filename = f"data/{task_name}_latest.json"
            os.makedirs("data", exist_ok=True)
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ Donn√©es sauvegard√©es pour {task_name} -> {filename}")
        except Exception as e:
            logger.warning(f"Impossible de sauvegarder le cache local pour {task_name}: {e}")

    def run_all(self, save_report: bool = True) -> Dict[str, Any]:
        """
        Ex√©cute les t√¢ches en parall√®le (contr√¥l√©), collecte les r√©sultats,
        construit un rapport consolid√© et le sauvegarde.
        """
        logger.info("=== D√âMARRAGE ORCHESTRATEUR ===")
        start_time = time.time()
        tasks = {
            "document_scraper": (self.document_scraper.scrape_bulletins, ()),
            "weather_service": (self.weather_service.scrape_forecast, ()),
            "sonagess_scraper": (self.sonagess_scraper.run, ()),
        }

        results: Dict[str, Dict[str, Any]] = {}
        durations: Dict[str, float] = {}

        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures_map = {}
            # soumettre toutes les t√¢ches
            for name, (fn, fn_args) in tasks.items():
                if self._stop_requested:
                    logger.warning("Arr√™t demand√© avant soumission de %s", name)
                    results[name] = {"status": "ERROR", "results": [], "error": "Stopped before start"}
                    continue
                logger.info("Soumission de la t√¢che %s", name)
                # on encapsule l'appel pour mesurer la dur√©e
                def _timed_call(f, *a, **kw):
                    t0 = time.time()
                    out = _safe_call_retryable(f, *a, **kw)
                    t1 = time.time()
                    return {"out": out, "duration": round(t1 - t0, 2)}
                futures_map[ex.submit(_timed_call, fn, *fn_args)] = name

            # r√©cup√©rer les r√©sultats au fur et √† mesure
            for fut in as_completed(futures_map):
                name = futures_map[fut]
                try:
                    res_wrapper = fut.result(timeout=TASK_TIMEOUT_S)
                    out = res_wrapper.get("out", {"status": "ERROR", "results": [], "error": "No output"})
                    dur = res_wrapper.get("duration", 0.0)
                    results[name] = _normalize_result(out)
                    durations[name] = dur
                    
                    # --- NOUVEAU : SAUVEGARDE SYST√âMATIQUE ---
                    self._save_local_cache(name, results[name])
                    
                    logger.info("T√¢che %s termin√©e en %.2fs (status=%s)", name, dur, results[name].get("status"))
                except TimeoutError:
                    logger.error("Timeout global pour la t√¢che %s", name)
                    results[name] = {"status": "ERROR", "results": [], "error": f"Timeout after {TASK_TIMEOUT_S}s"}
                    durations[name] = TASK_TIMEOUT_S
                except Exception as e:
                    logger.exception("Erreur lors de la r√©cup√©ration du r√©sultat de %s: %s", name, e)
                    results[name] = {"status": "ERROR", "results": [], "error": str(e)}
                if self._stop_requested:
                    logger.warning("Arr√™t demand√© : on arr√™te la collecte des r√©sultats restants.")
                    break

        total_time = round(time.time() - start_time, 2)

        # calcul du statut global
        statuses = [r.get("status", "ERROR") for r in results.values()]
        if all(s == "SUCCESS" for s in statuses):
            overall_status = "SUCCESS"
        elif all(s == "ERROR" for s in statuses):
            overall_status = "FAILURE"
        else:
            overall_status = "PARTIAL_SUCCESS"

        # compter documents collect√©s
        def _count(r):
            res = r.get("results", [])
            return len(res) if isinstance(res, list) else 0

        total_documents = sum(_count(r) for r in results.values())

        self.report = {
            "overall_status": overall_status,
            "timestamp": datetime.now().isoformat(),
            "total_duration_s": total_time,
            "task_durations_s": durations,
            "data_pipelines": results,
            "total_documents_collected": total_documents,
        }

        # sauvegarde du rapport
        if save_report:
            try:
                with open(REPORT_FILE, "w", encoding="utf-8") as f:
                    json.dump(self.report, f, ensure_ascii=False, indent=2)
                logger.info("Rapport sauvegard√©: %s", REPORT_FILE)
            except Exception as e:
                logger.warning("Impossible de sauvegarder le rapport: %s", e)

        logger.info("=== ORCHESTRATION TERMIN√âE : %s (%.2fs) ===", overall_status, total_time)
        return self.report


# Ex√©cution directe
if __name__ == "__main__":
    orch = ScraperOrchestrator(headless=True)
    final = orch.run_all(save_report=True)
    print(json.dumps(final, ensure_ascii=False, indent=2))