import logging
import os
import sys
import json
import time
from datetime import datetime
from typing import Dict, Any, List, Optional
# CORRECTION CRITIQUE: PlaywrightTimeoutError a √©t√© renomm√© en TimeoutError.
from playwright.sync_api import sync_playwright, Response, Page, TimeoutError, BrowserContext

# Configuration des logs (utilise le logger de l'orchestrateur)
logger = logging.getLogger("MapFeatureScraper")

# D√©finition de l'alias pour la compatibilit√© avec le reste du code
PlaywrightTimeoutError = TimeoutError

# --- CORRECTION CRITIQUE: Configuration par d√©faut en cas d'√©chec de l'import ---
try:
    # Tentative d'import de la configuration consolid√©e
    # Ceci est la structure attendue si le script est ex√©cut√© dans un framework plus large.
    from config import SCRAPER_CONFIG
    CONFIG_REFERENCE = SCRAPER_CONFIG
except ImportError:
    logger.critical("ATTENTION: config.py ou SCRAPER_CONFIG introuvable. Utilisation d'une configuration par d√©faut pour la d√©mo/revue.")
    CONFIG_REFERENCE = {
        'URL_MAP_VIEWER': "https://map.example.com/viewer", # URL de substitution si la config est absente
        'RAW_DATA_DIR': "raw_map_data",
        'HEADLESS_MODE': True,
        'BROWSER_TIMEOUT': 60000,
        'USER_AGENT': "Mozilla/5.0 (Playwright)",
        'NETWORK_IDLE_TIMEOUT': 25000, 
        'SELECTOR_MAP_MENU': ".datasets-menu, .map-layer-panel, .sidebar", # S√©lecteur par d√©faut du panneau de couches
        'WATCHLIST_MAP_CATEGORIES': ["Flood Risk", "Hydrology"],
        'SELECTOR_BLOCKERS': ['.modal', '.cookie-popup', '#overlay', '.leaflet-control-container'],
        'KEYWORDS_FANFAR_RISK': ["risk", "flood", "alert", "fanfar", "features"],
        'PROCESSOR_FLOOD_KEYWORDS': {}
    }
    # Cr√©ation du r√©pertoire de sortie pour le mock, n√©cessaire pour √©viter une erreur I/O
    if not os.path.exists(CONFIG_REFERENCE['RAW_DATA_DIR']):
        os.makedirs(CONFIG_REFERENCE['RAW_DATA_DIR'], exist_ok=True)
# ---------------------------------------------------------------------------------


class MapNetworkInterceptor:
    """
    Espion R√©seau : Capture les donn√©es vectorielles (GeoJSON).
    """
    def __init__(self, config: Dict[str, Any]):
        self.captured_features = []
        # Utilisation des mots-cl√©s de la config pour la d√©tection
        self.fanfar_keywords = config.get("KEYWORDS_FANFAR_RISK", ["features"])
        self.flood_keywords = config.get("PROCESSOR_FLOOD_KEYWORDS", {})

    def handle_response(self, response: Response):
        """Callback d√©clench√© √† chaque r√©ponse du serveur."""
        try:
            # 1. Filtre sur le type MIME JSON
            if "json" not in response.headers.get("content-type", ""):
                return

            # 2. Filtre sur l'URL pour cibler les couches de donn√©es
            url = response.url.lower()
            if not any(k in url for k in ["map", "layer", "feature", "dataset", "geojson"]):
                 # Inclure les mots-cl√©s de risque de Fanfar dans l'URL pour plus de robustesse
                 if not any(k in url for k in self.fanfar_keywords):
                     return

            data = response.json()
            
            # 3. V√©rification structure GeoJSON standard (FeatureCollection)
            is_geojson = isinstance(data, dict) and data.get("type") == "FeatureCollection"
            
            # 4. V√©rification alternative (structure de r√©ponse API de risque/Fanfar)
            # On v√©rifie si la r√©ponse JSON contient l'un des mots-cl√©s
            is_risk_data = any(k in json.dumps(data).lower() for k in self.fanfar_keywords)
            
            if is_geojson:
                features = data.get("features", [])
                
                # 5. Filtrage : On n'ajoute que les GeoJSON qui contiennent des features
                if features:
                    logger.info(f"üì° GeoJSON re√ßu ({len(features)} objets) via : {url[-40:]}")
                    self.captured_features.extend(features)
                    
            elif is_risk_data:
                # Si ce n'est pas un GeoJSON strict, on stocke la r√©ponse compl√®te si elle contient des donn√©es de risque
                logger.info(f"üì° Donn√©e Risque API re√ßue via : {url[-40:]}")
                # Enregistre la r√©ponse compl√®te avec l'URL en tant que m√©tadonn√©e
                self.captured_features.append({"metadata": url, "data": data})

        except Exception as e:
            # Souvent caus√© par une r√©ponse qui n'est pas du JSON valide, on ignore silencieusement
            # logger.debug(f"Erreur de lecture de r√©ponse JSON: {e}")
            pass

    def reset(self):
        self.captured_features = []

class MapController:
    """
    Le "Pilote" : G√®re l'interface et les blocages via les s√©lecteurs de config.
    """
    def __init__(self, page: Page, config: Dict[str, Any]):
        self.page = page
        self.config = config
        self.menu_selector = config.get('SELECTOR_MAP_MENU', ".datasets-menu")

    def nuke_overlays(self):
        """
        SOLUTION RADICALE : Supprime via JS tous les √©l√©ments bloquants d√©finis dans la config.
        """
        try:
            # R√©cup√©ration des s√©lecteurs de blocage depuis la configuration
            blockers = json.dumps(self.config.get('SELECTOR_BLOCKERS', []))
            
            self.page.evaluate(f"""() => {{
                const blockers = {blockers};
                blockers.forEach(selector => {{
                    document.querySelectorAll(selector).forEach(el => el.remove());
                }});
            }}""")
        except Exception as e:
            logger.warning(f"Erreur lors du nettoyage JS des overlays : {e}")

    def switch_category(self, category_name: str):
        """
        Clique sur un onglet de la palette pour charger la couche GeoJSON associ√©e.
        """
        logger.info(f"üëâ Recherche cat√©gorie : {category_name}")
        
        # 1. On nettoie le terrain avant de cliquer
        self.nuke_overlays()
        
        try:
            timeout = self.config.get('NETWORK_IDLE_TIMEOUT', 15000)
            
            # Cible : Texte insensible √† la casse
            btn = self.page.locator(f"text=/{category_name}/i").first
            
            # 2. Tentative de Clic
            if btn.count() > 0:
                # On force le clic pour ignorer les overlays invisibles
                btn.click(force=True, timeout=5000)
                logger.info(f"Clic forc√© sur '{category_name}' r√©ussi. Attente chargement...")
                # time.sleep(3) est conserv√© car l'activation de la couche peut prendre du temps
                # m√™me apr√®s l'√©v√©nement de clic et avant l'√©v√©nement r√©seau.
                time.sleep(3) 
            else:
                logger.error(f"Bouton '{category_name}' introuvable dans le menu.")
                
        except Exception as e:
            logger.error(f"√âchec critique du clic sur '{category_name}'. Re-nettoyage et tentative finale. Erreur: {e}")
            self.nuke_overlays()
            try:
                # Tentative finale de clic forc√©
                self.page.locator(f"text=/{category_name}/i").first.click(force=True, timeout=5000)
            except:
                pass


class MapFeatureScraper:
    """Service complet pour intercepter les donn√©es GeoJSON et autres features vectorielles."""

    def __init__(self, headless: bool = None):
        
        self.config = CONFIG_REFERENCE # Utilisation de la r√©f√©rence globale (mock ou import√©e)
            
        # Initialisation des variables de la classe
        self.headless = headless if headless is not None else self.config.get('HEADLESS_MODE', True)
        self.base_url = self.config.get('URL_MAP_VIEWER')
        self.output_dir = self.config.get('RAW_DATA_DIR')
        self.targets = self.config.get('WATCHLIST_MAP_CATEGORIES', [])
        
        if not self.base_url or not self.output_dir:
            logger.critical("Configuration manquante (URL_MAP_VIEWER ou RAW_DATA_DIR).")
            raise ValueError("Configuration de base manquante.")

        # Le r√©pertoire a d√©j√† √©t√© cr√©√© dans le bloc de configuration par d√©faut,
        # mais on assure ici qu'il existe si l'import initial a r√©ussi.
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir, exist_ok=True)


    def scrape_features(self) -> Dict[str, Any]:
        """Ex√©cute le scraping pour toutes les cat√©gories cibles."""
        logger.info(f"D√©marrage de l'interception de features GeoJSON pour {len(self.targets)} cibles.")
        
        all_features = {}
        successful_captures = 0

        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=self.headless, 
                args=["--no-sandbox", "--disable-setuid-sandbox"]
            )
            context = browser.new_context(
                viewport={"width": 1366, "height": 768},
                user_agent=self.config.get('USER_AGENT')
            )
            page = context.new_page()

            # Navigation initiale
            try:
                page.goto(self.base_url, wait_until="domcontentloaded", timeout=self.config.get('BROWSER_TIMEOUT'))
            except PlaywrightTimeoutError as e:
                logger.error(f"√âchec de l'acc√®s au site {self.base_url} (Timeout): {e}")
                browser.close()
                return {"status": "ERROR", "message": f"Navigation √©chou√©e: Timeout lors de l'acc√®s √† l'URL. {e}"}
            except Exception as e:
                logger.error(f"√âchec de l'acc√®s au site {self.base_url}: {e}")
                browser.close()
                return {"status": "ERROR", "message": f"Navigation √©chou√©e: {e}"}

            controller = MapController(page, self.config)
            
            # --- Attente explicite du menu de la carte pour √©viter les sleep() trop longs ---
            menu_selector = self.config.get('SELECTOR_MAP_MENU', ".datasets-menu, .map-layer-panel")
            try:
                page.wait_for_selector(menu_selector, state="visible", timeout=15000)
                logger.info("Menu de la carte d√©tect√©. Nettoyage initial des overlays.")
                controller.nuke_overlays()
                # Petite pause apr√®s le nettoyage pour laisser le DOM se stabiliser
                time.sleep(2) 
            except PlaywrightTimeoutError:
                logger.warning("Le menu de la carte n'est pas apparu dans les temps. Poursuite avec nettoyage forc√©.")
                controller.nuke_overlays()
            except Exception as e:
                logger.warning(f"Erreur lors de l'attente du menu/nettoyage : {e}")


            for category in self.targets:
                interceptor = MapNetworkInterceptor(self.config)
                
                # 1. Attacher l'intercepteur pour la cat√©gorie actuelle
                page.on("response", interceptor.handle_response)
                
                # 2. Changer de cat√©gorie (d√©clenche le chargement r√©seau)
                controller.switch_category(category)
                
                # 3. Attendre la stabilit√© du r√©seau pour capturer les donn√©es
                try:
                    # Utilisation d'un √©tat r√©seau stable
                    page.wait_for_load_state("networkidle", timeout=self.config.get('NETWORK_IDLE_TIMEOUT'))
                    time.sleep(3) # Attente suppl√©mentaire pour les requ√™tes asynchrones/rendering
                except PlaywrightTimeoutError:
                    logger.warning(f"Timeout r√©seau atteint pour {category}. Les donn√©es ont pu √™tre captur√©es m√™me sans 'networkidle'.")
                except Exception as e:
                    logger.warning(f"Erreur inattendue pendant l'attente r√©seau pour {category}: {e}")
                
                # 4. D√©tacher l'intercepteur (important pour la boucle)
                page.remove_listener("response", interceptor.handle_response)
                
                # 5. Sauvegarde des donn√©es captur√©es
                if interceptor.captured_features:
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = os.path.join(self.output_dir, f"{category.lower().replace(' ', '_')}_{timestamp}.geojson")
                    
                    # Assurer que les features sont dans une FeatureCollection valide avant la sauvegarde
                    # Filtration pour s'assurer que seuls les dictionnaires de type Feature sont inclus, 
                    # ignorant les objets de m√©tadonn√©es brutes que l'intercepteur peut avoir stock√©s.
                    geojson_data = {
                        "type": "FeatureCollection",
                        "features": [f for f in interceptor.captured_features if isinstance(f, dict) and f.get('type') == 'Feature'], 
                        "metadata": {
                            "source_url": self.base_url,
                            "category": category,
                            "captured_items": len(interceptor.captured_features),
                            "timestamp": timestamp
                        }
                    }
                    
                    with open(filename, 'w', encoding='utf-8') as f:
                        json.dump(geojson_data, f, ensure_ascii=False, indent=2)
                        
                    all_features[category] = filename
                    successful_captures += 1
                    logger.info(f"üíæ {len(geojson_data['features'])} features GeoJSON sauvegard√©es pour '{category}' dans : {filename}")
                else:
                    logger.warning(f"Aucune feature GeoJSON captur√©e pour '{category}'.")
            
            browser.close()

        if successful_captures > 0:
            return {
                "status": "SUCCESS",
                "message": f"{successful_captures} couches de features GeoJSON captur√©es.",
                "results": all_features # Dictionnaire {cat√©gorie: chemin_fichier}
            }
        else:
            return {
                "status": "FAILURE",
                "message": "Aucune donn√©e GeoJSON n'a pu √™tre captur√©e pour les cibles d√©finies.",
                "results": {}
            }