from typing import Any,List,Dict

import json
import logging
import re


logger = logging.getLogger("FormationCoachTool")

class FormationTool:
    def __init__(self,llm,model_planner="llama-3.3-70b-versatile",model_answer="llama-3.3-70b-versatile"    ):
        self.llm = llm
        self.model_planner = model_planner
        self.model_answer = model_answer    

    def _extract_json_block(self, text: str) -> Dict[str, Any]:
        matches = re.findall(r"\{[\s\S]*?\}", text)
        for match in matches:
            try:
                return json.loads(match)
            except json.JSONDecodeError:
                continue
        return json.loads(text)

    def _plan_retrieval(self, query: str, profile: Dict[str, Any]) -> Dict[str, Any]:
        fallback = {
            "optimized_query": query,
            "modules": [],
            "prerequisites": [],
            "reasoning": "",
            "warnings": [],
        }

        if not self.llm:
            fallback["warnings"].append("LLM indisponible pour planifier la recherche.")
            return fallback

        profile_text = self._format_profile(profile)
        planner_prompt = (
            "Tu es l'orchestrateur p√©dagogique d'AgriConnect. "
            "Analyse la question suivante et pr√©pare une recherche RAG.\n"
            f"Profil apprenant : {profile_text}\n"
            f"Question : {query}\n\n"
            'R√©ponds en JSON avec : {"optimized_query": "...", "modules": ["..."], '
            '"prerequisites": ["..."], "reasoning": "..."}'
        )

        try:
            completion = self.llm.chat.completions.create(
                model=self.model_planner,
                messages=[{"role": "user", "content": planner_prompt}],
                temperature=0.2,
                max_tokens=400,
                response_format={"type": "json_object"},
            )
            content = completion.choices[0].message.content
            if not content:
                raise ValueError("R√©ponse vide du planificateur.")
            plan = json.loads(content)
            return {
                "optimized_query": plan.get("optimized_query") or query,
                "modules": plan.get("modules", []),
                "prerequisites": plan.get("prerequisites", []),
                "reasoning": plan.get("reasoning", ""),
                "warnings": [],
            }
        except Exception as exc:
            logger.warning("Planification RAG impossible : %s", exc)
            fallback["warnings"].append("Planification RAG automatique indisponible.")
            return fallback

    def _build_context(self, nodes: List[Any]) -> str:
        sections = []
        for idx, node in enumerate(nodes, start=1):
            metadata = node.node.metadata or {}
            label = metadata.get("title") or metadata.get("filename") or f"Source {idx}"
            chunk = node.node.get_content().strip()
            sections.append(f"[Source {idx} | {label}]\n{chunk}")
        return "\n\n".join(sections)

    def _serialize_sources(self, nodes: List[Any]) -> List[Dict[str, Any]]:
        payload: List[Dict[str, Any]] = []
        for idx, node in enumerate(nodes, start=1):
            metadata = node.node.metadata or {}
            payload.append(
                {
                    "index": idx,
                    "title": metadata.get("title"),
                    "filename": metadata.get("filename"),
                    "score": float(node.score) if node.score is not None else None,
                }
            )
        return payload

    def _format_profile(self, profile: Dict[str, Any]) -> str:
        if not profile:
            return "Non renseign√©"
        parts: List[str] = []
        for key, value in profile.items():
            if value in (None, "", []):
                continue
            parts.append(f"{key}: {value}")
        return "; ".join(parts) if parts else "Non renseign√©"

    def _analyze_request(self, query: str, profile: Dict[str, Any]) -> Dict[str, Any]:
        fallback = {
            "intent": "FORMATION",
            "focus_topics": [],
            "field_actions": [],
            "safety_flags": [],
            "urgency": "NORMAL",
            "warnings": [],
        }

        if not self.llm:
            fallback["warnings"].append("LLM indisponible pour analyser la demande.")
            return fallback

        profile_text = self._format_profile(profile)
        analyzer_prompt = (
            "Tu es l'ing√©nieur p√©dagogique expert d'AgriConnect. Ton r√¥le est de qualifier la demande de l'utilisateur "
            "pour optimiser la recherche documentaire (RAG) et garantir la s√©curit√© des conseils.\n\n"
            
            f"PROFIL APPRENANT : {profile_text}\n"
            f"QUESTION : {query}\n\n"
            
            "CONSIGNES DE G√âN√âRATION JSON :\n"
            "1. is_relevant : (Boolean) True si la question concerne l'agriculture, l'√©levage, la m√©t√©o agricole ou la formation. False pour tout sujet hors-domaine (sport, politique, cuisine non-agricole, etc.).\n"
            "2. rejection_reason : (String) Si is_relevant=False, explique poliment pourquoi tu ne peux pas r√©pondre (en restant dans ton r√¥le d'expert agricole).\n"
            "3. intent : Choisir parmi [FORMATION, URGENCE, CONSEIL].\n"
            "4. focus_topics : Liste de mots-cl√©s optimis√©s pour une recherche s√©mantique (ex: 'entretien culture ni√©b√©', 'lutte chenilles').\n"
            "5. field_actions : Liste les cat√©gories techniques √† v√©rifier dans les documents (ex: 'densit√© de semis', 'dosage engrais'). Ne donne JAMAIS de chiffres ou de m√©thodes √† ce stade.\n"
            "6. safety_flags : Identifie les risques critiques (ex: 'toxicit√© pesticides', 'sant√© animale', '√©rosion') n√©cessitant une attention particuli√®re.\n"
            "7. urgency : Choisir selon l'impact sur la r√©colte : [NORMAL, HAUTE, CRITIQUE].\n\n"
            
            "R√âPONDS UNIQUEMENT SOUS CE FORMAT JSON :\n"
            "{\n"
            '  "is_relevant": true,\n'
            '  "rejection_reason": "",\n'
            '  "intent": "...",\n'
            '  "focus_topics": [],\n'
            '  "field_actions": [],\n'
            '  "safety_flags": [],\n'
            '  "urgency": "...",\n'
            '  "warnings": []\n'
            "}"
        )

        try:
            completion = self.llm.chat.completions.create(
                model=self.model_planner,
                messages=[{"role": "user", "content": analyzer_prompt}],
                temperature=0.1,
                max_tokens=300,
                response_format={"type": "json_object"},
            )
            content = completion.choices[0].message.content
            if not content:
                raise ValueError("R√©ponse vide de l'analyseur.")
            
            analysis = json.loads(content)
            return {
                "is_relevant": analysis.get("is_relevant", True), 
                "rejection_reason": analysis.get("rejection_reason", ""),
                "intent": analysis.get("intent", "FORMATION"),
                "focus_topics": analysis.get("focus_topics", []),
                "field_actions": analysis.get("field_actions", []),
                "safety_flags": analysis.get("safety_flags", []),
                "urgency": analysis.get("urgency", "NORMAL"),
                "warnings": analysis.get("warnings", []),
            }
        except Exception as e:
            logger.warning("Analyse de requ√™te impossible : %s", e)
            fallback["warnings"].append("Analyse de requ√™te automatique indisponible.")
            return fallback
     
    def _fallback_answer(
        self,
        query: str,
        profile_text: str,
        prerequisites: List[str],
        modules: List[str],
        sources: List[Dict[str, Any]],
    ) -> str:
        # Construction d'un texte propre pour les sources
        source_titles = []
        for s in sources:
            title = s.get("title") or s.get("filename") or f"Source {s.get('index')}"
            source_titles.append(title)
        
        sources_text = ", ".join(source_titles) if source_titles else "Fiches techniques locales"

        return (
            "D√©sol√©, je rencontre une difficult√© technique momentan√©e pour g√©n√©rer une r√©ponse d√©taill√©e.\n\n"
            "Cependant, voici les ressources identifi√©es pour vous aider :\n\n"
            f"‚ùì **Question** : {query}\n"
            f"üìö **Sujet** : {', '.join(modules) if modules else 'Agriculture g√©n√©rale'}\n"
            f"üìÑ **Documents trouv√©s** : {sources_text}\n\n"
            "Conseil : Vous pouvez consulter ces documents ou reformuler votre question."
        )

